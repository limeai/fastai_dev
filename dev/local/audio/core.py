#AUTOGENERATED! DO NOT EDIT! File to edit: dev/70_audio_core.ipynb (unless otherwise specified).

__all__ = ['audio_extensions', 'get_audio_files', 'AudioGetter', 'load_audio', 'AudioSignal',
           'AudioSpectrogram', 'MelSpectrogramify', 'AmplitudeToDBify', 'MFCCify', 'downmix', 'Resample', 'resize',
           'RemoveType', 'remove_silence', 'apply_rules']

#Cell
from ..torch_basics import *
from ..test import *
from ..data.all import *
from ..notebook.showdoc import *

#Cell
from ..torch_basics import *
from ..test import *
from ..layers import *
from ..data.all import *
from ..notebook.showdoc import show_doc
from ..optimizer import *
from ..learner import *
from ..callback.progress import *

#Cell
from IPython.display import Audio
import torchaudio
from torchaudio.transforms import MelSpectrogram, MFCC, AmplitudeToDB
from scipy.signal import resample_poly
from librosa.effects import trim,split

#Cell
URLs.ESC50_SAMPLE = 'https://github.com/limeai/dataset/raw/master/esc50_sample.tgz'

#Cell
audio_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('audio/'))

#Cell
def get_audio_files(path, recurse=True, folders=None):
    "Get audio files in `path` recursively."
    return get_files(path, extensions=audio_extensions, recurse=recurse, folders=folders)

#Cell
def AudioGetter(suf='', recurse=True, folders=None):
    "Create `get_audio_files` partial function that searches path suffix `suf` and passes along `kwargs`"
    def _inner(o, recurse=recurse, folders=folders): return get_audio_files(o/suf, recurse, folders)
    return _inner

#Cell
def load_audio(fn, **kwargs):
    "Open and load an audio file"
    signal, sample_rate = torchaudio.load(fn)
    return signal, sample_rate

#Cell
class AudioSignal(Tuple):
    "Basic type for an audio signal"
    def show(self, ctx=None, **kwargs):
        return display(Audio(data=self.signal, rate=self.sample_rate))

    @classmethod
    def create(cls, fn:Path): return cls(load_audio(fn))

    signal,sample_rate = add_props(lambda i,self: self[i])

    @property
    def n_ch(self):
        "Number of channels"
        return self.signal.shape[0]

#Cell
class AudioSpectrogram(TensorImageBase): pass

#Cell
@delegates(MelSpectrogram)
class MelSpectrogramify(Transform):
    def __init__(self, **kwargs):
        self.transformer = MelSpectrogram(**kwargs)
    def encodes(self, x:AudioSignal): return AudioSpectrogram(self.transformer(x.signal).detach())

#Cell
@delegates(AmplitudeToDB)
class AmplitudeToDBify(Transform):
    def __init__(self, **kwargs):
        self.transformer = AmplitudeToDB(**kwargs)
    def encodes(self, x:AudioSpectrogram):
        return self.transformer(x)
    def encodes(self, x:AudioSignal):
        db_signal = self.transformer(x.signal)
        return AudioSignal(db_signal, x.sample_rate)

#Cell
@delegates(MFCC)
class MFCCify(Transform):
    def __init__(self, **kwargs):
        print(kwargs)
        self.transformer = MFCC(**kwargs)
    def encodes(self, x:AudioSignal): return AudioSpectrogram(self.transformer(x.signal).detach())

#Cell
def downmix(audio:AudioSignal)->AudioSignal:
    "Downmix any stereo signals to mono."
    return AudioSignal(torch.mean(audio.signal.float(),0,keepdim=True),audio.sample_rate)

#Cell
def Resample(new_sample_rate):
    '''Resample using faster polyphase technique and avoid FFT computation'''
    def _inner(audio, new_sample_rate=new_sample_rate):
        if audio.sample_rate == new_sample_rate: return audio
        return AudioSignal(resample_poly(audio.signal, new_sample_rate, audio.sample_rate, axis=-1),new_sample_rate)
    return _inner

#Cell
def resize(): pass # Kevin Pad or Crop to make sample size equal

#Cell
from enum import auto, Enum
class RemoveType(Enum):
    TRIM, ALL = auto(),auto()

#Cell
def _get_signal_interval(signal, interval_idxs):
    full_index = [slice(None)] * signal.ndim
    full_index[-1] = slice(interval_idxs[0], interval_idxs[1])
    return signal[tuple(full_index)]

#Cell
def _merge_splits(signal, intervals, pad_samples):
    silence = np.zeros(signal.shape[:-1] + (pad_samples,))
    for i, interval_idxs in enumerate(intervals):
        signal_interval = _get_signal_interval(signal, interval_idxs)
        if i == 0:
            merged = signal_interval
        else:
            merged = np.concatenate((merged, silence, signal_interval), axis=-1)
    return merged

#Cell
def remove_silence(audio:AudioSignal, remove_type:RemoveType, threshold_db=20, pad_ms=200):
    pad_samples = int(pad_ms/1000*audio.sample_rate)
    original = audio.signal.numpy()
    if remove_type == RemoveType.TRIM:
        trimmed, _ = trim(original, top_db=threshold_db, hop_length=pad_samples)
        return AudioSignal(torch.from_numpy(trimmed), audio.sample_rate)
    else: # remove_type == RemoveType.ALL
        splits = split(original, top_db=threshold_db, hop_length=pad_samples)
        trimmed = _merge_splits(original, splits, pad_samples)
        return AudioSignal(torch.from_numpy(trimmed), audio.sample_rate)

#Cell
def apply_rules(items, rules): #TODO: Kevin Consolidate with 30_text_core apply_rules elsewhere
    "Returns a generator that apply `rules`  to `items`"
    return map(compose(*rules), items)