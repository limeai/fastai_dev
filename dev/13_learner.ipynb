{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.layers import *\n",
    "from local.data.all import *\n",
    "from local.optimizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.notebook.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_all_ = ['CancelFitException', 'CancelEpochException', 'CancelTrainException', 'CancelValidException', 'CancelBatchException']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "> Basic class for handling the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following for testing purposes (a basic linear regression problem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def synth_dbunch(a=2, b=3, bs=16, n_train=10, n_valid=2, cuda=False):\n",
    "    def get_data(n):\n",
    "        x = torch.randn(int(bs*n))\n",
    "        return TensorDataset(x, a*x + b + 0.1*torch.randn(int(bs*n)))\n",
    "    train_ds = get_data(n_train)\n",
    "    valid_ds = get_data(n_valid)\n",
    "    tfms = Cuda() if cuda else None\n",
    "    train_dl = TfmdDL(train_ds, bs=bs, shuffle=True, after_batch=tfms, num_workers=0)\n",
    "    valid_dl = TfmdDL(valid_ds, bs=bs, after_batch=tfms, num_workers=0)\n",
    "    return DataBunch(train_dl, valid_dl)\n",
    "\n",
    "class RegModel(Module):\n",
    "    def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n",
    "    def forward(self, x): return x*self.a + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback(GetAttr):\n",
    "    \"Basic class handling tweaks of the training loop by changing a `Learner` in various events\"\n",
    "    _default,learn = 'learn',None\n",
    "    def __repr__(self): return type(self).__name__\n",
    "\n",
    "    def __call__(self, event_name):\n",
    "        \"Call `self.{event_name}` if it's defined\"\n",
    "        getattr(self, event_name, noop)()\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"Name of the `Callback`, camel-cased and with '*Callback*' removed\"\n",
    "        return class2attr(self, 'Callback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is defined in `Learner` a bit below and consists in a minimal set of instructions: looping through the data we:\n",
    "- compute the output of the model from the input\n",
    "- calculate a loss between this output and the desired target\n",
    "- compute the gradients of this loss with respect to all the model parameters\n",
    "- update the parameters accordingly\n",
    "- zero all the gradients\n",
    "\n",
    "Any tweak of this training loop is defined in a `Callback` to avoid over-complicating the code of the training loop, and to make it easy to mix and match different techniques (since they'll be defined in different callbacks). A callback can implement actions on the following events:\n",
    "\n",
    "- `begin_fit`: called before doing anything, ideal for initial setup.\n",
    "- `begin_epoch`: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch.\n",
    "- `begin_train`: called at the beginning of the training part of an epoch.\n",
    "- `begin_batch`: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance).\n",
    "- `after_pred`: called after computing the output of the model on the batch. It can be used to change that output before it's fed to the loss.\n",
    "- `after_loss`: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance).\n",
    "- `after_backward`: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance).\n",
    "- `after_step`: called after the step and before the gradients are zeroed.\n",
    "- `after_batch`: called at the end of a batch, for any clean-up before the next one.\n",
    "- `after_train`: called at the end of the training phase of an epoch.\n",
    "- `begin_validate`: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation.\n",
    "- `after_validate`: called at the end of the validation part of an epoch.\n",
    "- `after_epoch`: called at the end of an epoch, for any clean-up before the next one.\n",
    "- `after_fit`: called at the end of training, for final clean-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Callback.__call__\" class=\"doc_header\"><code>Callback.__call__</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Callback.__call__</code>(**`event_name`**)\n",
       "\n",
       "Call `self.{event_name}` if it's defined"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Callback.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_cb = Callback()\n",
    "tst_cb.call_me = lambda: print(\"maybe\")\n",
    "test_stdout(lambda: tst_cb(\"call_me\"), \"maybe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GetAttr.__getattr__\" class=\"doc_header\"><code>GetAttr.__getattr__</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/local/core.py#L204\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GetAttr.__getattr__</code>(**`k`**)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Callback.__getattr__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a shortcut to avoid having to write `self.learn.bla` for any `bla` attribute we seek, and just write `self.bla`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_class('TstLearner', 'a')\n",
    "\n",
    "class TstCallback(Callback):\n",
    "    def batch_begin(self): print(self.a)\n",
    "\n",
    "learn,cb = TstLearner(1),TstCallback()\n",
    "cb.learn = learn\n",
    "test_stdout(lambda: cb('batch_begin'), \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it only works to get the value of the attribute, if you want to change it, you have to manually access it with `self.learn.bla`. In the example below, `self.a += 1` creates an `a` attribute of 2 in the callback instead of setting the `a` of the learner to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TstCallback(Callback):\n",
    "    def batch_begin(self): self.a += 1\n",
    "\n",
    "learn,cb = TstLearner(1),TstCallback()\n",
    "cb.learn = learn\n",
    "cb('batch_begin')\n",
    "test_eq(cb.a, 2)\n",
    "test_eq(cb.learn.a, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proper version needs to write `self.learn.a = self.a + 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TstCallback(Callback):\n",
    "    def batch_begin(self): self.learn.a = self.a + 1\n",
    "\n",
    "learn,cb = TstLearner(1),TstCallback()\n",
    "cb.learn = learn\n",
    "cb('batch_begin')\n",
    "test_eq(cb.learn.a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Callback.name\" class=\"doc_header\"><code>Callback.name</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Name of the [`Callback`](/learner.html#Callback), camel-cased and with '*Callback*' removed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Callback.name, name='Callback.name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(TstCallback().name, 'tst')\n",
    "class ComplicatedNameCallback(Callback): pass\n",
    "test_eq(ComplicatedNameCallback().name, 'complicated_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainEvalCallback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainEvalCallback(Callback):\n",
    "    \"`Callback` that tracks the number of iterations done and properly sets training/eval mode\"\n",
    "    def begin_fit(self):\n",
    "        \"Set the iter and epoch counters to 0, put the model and the right device\"\n",
    "        self.learn.train_iter,self.learn.pct_train = 0,0.\n",
    "        self.model.to(self.dbunch.device)\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Update the iter counter (in training mode)\"\n",
    "        if not self.training: return\n",
    "        self.learn.pct_train += 1./(self.n_iter*self.n_epoch)\n",
    "        self.learn.train_iter   += 1\n",
    "\n",
    "    def begin_train(self):\n",
    "        \"Set the model in training mode\"\n",
    "        self.learn.pct_train=self.epoch/self.n_epoch\n",
    "        self.model.train()\n",
    "        self.learn.training=True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        \"Set the model in validation mode\"\n",
    "        self.model.eval()\n",
    "        self.learn.training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"TrainEvalCallback\" class=\"doc_header\"><code>class</code> <code>TrainEvalCallback</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>TrainEvalCallback</code>() :: [`Callback`](/learner.html#Callback)\n",
       "\n",
       "[`Callback`](/learner.html#Callback) that tracks the number of iterations done and properly sets training/eval mode"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TrainEvalCallback, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Callback` is automatically added in every `Learner` at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test of the TrainEvalCallback below in Learner.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TrainEvalCallback.begin_fit\" class=\"doc_header\"><code>TrainEvalCallback.begin_fit</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TrainEvalCallback.begin_fit</code>()\n",
       "\n",
       "Set the iter and epoch counters to 0, put the model and the right device"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TrainEvalCallback.begin_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TrainEvalCallback.after_batch\" class=\"doc_header\"><code>TrainEvalCallback.after_batch</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TrainEvalCallback.after_batch</code>()\n",
       "\n",
       "Update the iter counter (in training mode)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TrainEvalCallback.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TrainEvalCallback.begin_train\" class=\"doc_header\"><code>TrainEvalCallback.begin_train</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TrainEvalCallback.begin_train</code>()\n",
       "\n",
       "Set the model in training mode"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TrainEvalCallback.begin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TrainEvalCallback.begin_validate\" class=\"doc_header\"><code>TrainEvalCallback.begin_validate</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TrainEvalCallback.begin_validate</code>()\n",
       "\n",
       "Set the model in validation mode"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TrainEvalCallback.begin_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GatherPredsCallback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GatherPredsCallback(Callback):\n",
    "    \"`Callback` that saves the predictions and targets, optionally `with_loss`\"\n",
    "    def __init__(self, with_input=False, with_loss=False): store_attr(self, \"with_input,with_loss\")\n",
    "\n",
    "    def begin_batch(self):\n",
    "        if self.with_input: self.inputs.append((to_detach(self.xb)))\n",
    "            \n",
    "    def begin_validate(self):\n",
    "        \"Initialize containers\"\n",
    "        self.preds,self.targets = [],[]\n",
    "        if self.with_input: self.inputs=[]\n",
    "        if self.with_loss: self.losses = []\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Save predictions, targets and potentially losses\"\n",
    "        self.preds.append(to_detach(self.pred))\n",
    "        self.targets.append(to_detach(self.yb))\n",
    "        if self.with_loss:  self.losses.append(to_detach(self.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"GatherPredsCallback\" class=\"doc_header\"><code>class</code> <code>GatherPredsCallback</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>GatherPredsCallback</code>(**`with_input`**=*`False`*, **`with_loss`**=*`False`*) :: [`Callback`](/learner.html#Callback)\n",
       "\n",
       "[`Callback`](/learner.html#Callback) that saves the predictions and targets, optionally `with_loss`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GatherPredsCallback, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GatherPredsCallback.begin_validate\" class=\"doc_header\"><code>GatherPredsCallback.begin_validate</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GatherPredsCallback.begin_validate</code>()\n",
       "\n",
       "Initialize containers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GatherPredsCallback.begin_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GatherPredsCallback.after_batch\" class=\"doc_header\"><code>GatherPredsCallback.after_batch</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GatherPredsCallback.after_batch</code>()\n",
       "\n",
       "Save predictions, targets and potentially losses"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GatherPredsCallback.after_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks control flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It happens that we may want to skip some of the steps of the training loop: in gradient accumulation, we don't aways want to do the step/zeroing of the grads for instance. During an LR finder test, we don't want to do the validation phase of an epoch. Or if we're training with a strategy of early stopping, we want to be able to completely interrupt the training loop.\n",
    "\n",
    "This is made possible by raising specific exceptions the training loop will look for (and properly catch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_ex_docs = dict(\n",
    "    CancelFitException=\"Skip the rest of this batch and go to `after_batch`\",\n",
    "    CancelEpochException=\"Skip the rest of the training part of the epoch and go to `after_train`\",\n",
    "    CancelTrainException=\"Skip the rest of the validation part of the epoch and go to `after_validate`\",\n",
    "    CancelValidException=\"Skip the rest of this epoch and go to `after_epoch`\",\n",
    "    CancelBatchException=\"Interrupts training and go to `after_fit`\")\n",
    "\n",
    "for c,d in _ex_docs.items(): mk_class(c,sup=Exception,doc=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"CancelBatchException\" class=\"doc_header\"><code>class</code> <code>CancelBatchException</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>CancelBatchException</code>(**\\*`args`**, **\\*\\*`kwargs`**) :: `Exception`\n",
       "\n",
       "Interrupts training and go to `after_fit`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CancelBatchException, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"CancelTrainException\" class=\"doc_header\"><code>class</code> <code>CancelTrainException</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>CancelTrainException</code>(**\\*`args`**, **\\*\\*`kwargs`**) :: `Exception`\n",
       "\n",
       "Skip the rest of the validation part of the epoch and go to `after_validate`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CancelTrainException, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"CancelValidException\" class=\"doc_header\"><code>class</code> <code>CancelValidException</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>CancelValidException</code>(**\\*`args`**, **\\*\\*`kwargs`**) :: `Exception`\n",
       "\n",
       "Skip the rest of this epoch and go to `after_epoch`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CancelValidException, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"CancelEpochException\" class=\"doc_header\"><code>class</code> <code>CancelEpochException</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>CancelEpochException</code>(**\\*`args`**, **\\*\\*`kwargs`**) :: `Exception`\n",
       "\n",
       "Skip the rest of the training part of the epoch and go to `after_train`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CancelEpochException, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"CancelFitException\" class=\"doc_header\"><code>class</code> <code>CancelFitException</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>CancelFitException</code>(**\\*`args`**, **\\*\\*`kwargs`**) :: `Exception`\n",
       "\n",
       "Skip the rest of this batch and go to `after_batch`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CancelFitException, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can detect one of those exceptions occurred and add code that executes right after with the following events:\n",
    "- `after_cancel_batch`: reached imediately after a `CancelBatchException` before proceeding to `after_batch`\n",
    "- `after_cancel_train`: reached imediately after a `CancelTrainException` before proceeding to `after_epoch`\n",
    "- `after_cancel_valid`: reached imediately after a `CancelValidException` before proceeding to `after_epoch`\n",
    "- `after_cancel_epoch`: reached imediately after a `CancelEpochException` before proceeding to `after_epoch`\n",
    "- `after_cancel_fit`: reached imediately after a `CancelFitException` before proceeding to `after_fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_events = L.split('begin_fit begin_epoch begin_train begin_batch after_pred after_loss \\\n",
    "    after_backward after_step after_cancel_batch after_batch after_cancel_train \\\n",
    "    after_train begin_validate after_cancel_validate after_validate after_cancel_epoch \\\n",
    "    after_epoch after_cancel_fit after_fit')\n",
    "\n",
    "mk_class('event', **_events.map_dict(),\n",
    "         doc=\"All possible events as attributes to get tab-completion and typo-proofing\")\n",
    "\n",
    "_before_epoch = [event.begin_fit, event.begin_epoch]\n",
    "_after_epoch  = [event.after_epoch, event.after_fit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_all_ = ['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"event\" class=\"doc_header\"><code>class</code> <code>event</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>event</code>(**\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "All possible events as attributes to get tab-completion and typo-proofing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(event, name='event', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(event.after_backward, 'after_backward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the full list: *begin_fit begin_epoch begin_train begin_batch after_pred after_loss after_backward after_step after_cancel_batch after_batch after_cancel_train after_train begin_validate after_cancel_validate after_validate after_cancel_epoch after_epoch after_cancel_fit after_fit*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Full test of the control flow below, after the Learner class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "defaults.lr = slice(3e-3)\n",
    "defaults.wd = 1e-2\n",
    "defaults.callbacks = [TrainEvalCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def replacing_yield(o, attr, val):\n",
    "    \"Context manager to temporarily replace an attribute\"\n",
    "    old = getattr(o,attr)\n",
    "    try:     yield setattr(o,attr,val)\n",
    "    finally: setattr(o,attr,old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mk_metric(m):\n",
    "    \"Convert `m` to an `AvgMetric`, unless it's already a `Metric`\"\n",
    "    return m if isinstance(m, Metric) else AvgMetric(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_model(file, model, opt, with_opt=True):\n",
    "    \"Save `model` to `file` along with `opt` (if available, and if `with_opt`)\"\n",
    "    if opt is None: with_opt=False\n",
    "    state = get_model(model).state_dict()\n",
    "    if with_opt: state = {'model': state, 'opt':opt.state_dict()}\n",
    "    torch.save(state, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_model(file, model, opt, with_opt=None, device=None, strict=True):\n",
    "    \"Load `model` from `file` along with `opt` (if available, and if `with_opt`)\"\n",
    "    if isinstance(device, int): device = torch.device('cuda', device)\n",
    "    state = torch.load(file)\n",
    "    hasopt = set(state)=={'model', 'opt'}\n",
    "    model_state = state['model'] if hasopt else state\n",
    "    get_model(model).load_state_dict(model_state, strict=strict)\n",
    "    if hasopt and ifnone(with_opt,True):\n",
    "        try: opt.load_state_dict(state['opt'])\n",
    "        except:\n",
    "            if with_opt: warn(\"Could not load the optimizer state.\")\n",
    "    elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(tensor([1]),),(tensor([2]),),(tensor([3]),)]\n",
    "y = [(tensor([1]),tensor([1])),(tensor([2]),tensor([2])),(tensor([3]),tensor([3]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Learner():\n",
    "    def __init__(self, dbunch, model, loss_func=None, opt_func=SGD, lr=defaults.lr, splitter=trainable_params, cbs=None,\n",
    "                 cb_funcs=None, metrics=None, path=None, model_dir='models', wd_bn_bias=False, train_bn=True):\n",
    "        store_attr(self, \"dbunch,model,opt_func,lr,splitter,model_dir,wd_bn_bias,train_bn\")\n",
    "        self.training,self.logger,self.opt,self.cbs = False,print,None,L()\n",
    "        #TODO: infer loss_func from data\n",
    "        if loss_func is None:\n",
    "            loss_func = getattr(dbunch.train_ds, 'loss_func', None)\n",
    "            assert loss_func is not None, \"Could not infer loss function from the data, please pass a loss function.\"\n",
    "        self.loss_func = loss_func\n",
    "        self.path = path if path is not None else getattr(dbunch, 'path', Path('.'))\n",
    "        self.metrics = L(metrics).map(mk_metric)\n",
    "        self.add_cbs(cbf() for cbf in L(defaults.callbacks)+L(cb_funcs))\n",
    "        self.add_cbs(cbs)\n",
    "        self.model.to(self.dbunch.device)\n",
    "\n",
    "    def add_cbs(self, cbs): L(cbs).map(self.add_cb)\n",
    "    def remove_cbs(self, cbs): L(cbs).map(self.remove_cb)\n",
    "    def add_cb(self, cb):\n",
    "        old = getattr(self, cb.name, None)\n",
    "        assert not old or isinstance(old, type(cb)), f\"self.{cb.name} already registered\"\n",
    "        cb.learn = self\n",
    "        setattr(self, cb.name, cb)\n",
    "        self.cbs.append(cb)\n",
    "        return self\n",
    "\n",
    "    def remove_cb(self, cb):\n",
    "        cb.learn = None\n",
    "        if hasattr(self, cb.name): delattr(self, cb.name)\n",
    "        if cb in self.cbs: self.cbs.remove(cb)\n",
    "\n",
    "    @contextmanager\n",
    "    def added_cbs(self, cbs):\n",
    "        self.add_cbs(cbs)\n",
    "        yield\n",
    "        self.remove_cbs(cbs)\n",
    "\n",
    "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
    "    def _call_one(self, event_name):\n",
    "        assert hasattr(event, event_name)\n",
    "        [cb(event_name) for cb in sort_by_run(self.cbs)]\n",
    "\n",
    "    def _bn_bias_state(self, with_bias): return bn_bias_params(self.model, with_bias).map(self.opt.state)\n",
    "    def create_opt(self):\n",
    "        self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n",
    "        if not self.wd_bn_bias:\n",
    "            for p in self._bn_bias_state(True ): p['do_wd'] = False\n",
    "        if self.train_bn:\n",
    "            for p in self._bn_bias_state(False): p['force_train'] = True\n",
    "\n",
    "    def _split(self, b): \n",
    "        i = getattr(self.dbunch, 'n_inp', 1 if len(b)==1 else len(b)-1)\n",
    "        self.xb,self.yb = b[:i],b[i:]\n",
    "    \n",
    "    def all_batches(self):\n",
    "        self.n_iter = len(self.dl)\n",
    "        for o in enumerate(self.dl): self.one_batch(*o)\n",
    "\n",
    "    def one_batch(self, i, b):\n",
    "        self.iter = i\n",
    "        try:\n",
    "            self._split(b);                                  self('begin_batch')\n",
    "            self.pred = self.model(*self.xb);                self('after_pred')\n",
    "            if len(self.yb) == 0: return\n",
    "            self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n",
    "            if not self.training: return\n",
    "            self.loss.backward();                            self('after_backward')\n",
    "            self.opt.step();                                 self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException:                         self('after_cancel_batch')\n",
    "        finally:                                             self('after_batch')\n",
    "\n",
    "    def _do_begin_fit(self, n_epoch):\n",
    "        self.n_epoch,self.loss = n_epoch,tensor(0.);         self('begin_fit')\n",
    "\n",
    "    def _do_epoch_train(self):\n",
    "        try:\n",
    "            self.dl = self.dbunch.train_dl;                  self('begin_train')\n",
    "            self.all_batches()\n",
    "        except CancelTrainException:                         self('after_cancel_train')\n",
    "        finally:                                             self('after_train')\n",
    "\n",
    "    def _do_epoch_validate(self, ds_idx=1, dl=None):\n",
    "        if dl is None: dl = self.dbunch.dls[ds_idx]\n",
    "        try:\n",
    "            self.dl = dl;                                    self('begin_validate')\n",
    "            with torch.no_grad(): self.all_batches()\n",
    "        except CancelValidException:                         self('after_cancel_validate')\n",
    "        finally:                                             self('after_validate')\n",
    "\n",
    "    def fit(self, n_epoch, lr=None, wd=defaults.wd, cbs=None, reset_opt=False):\n",
    "        with self.added_cbs(cbs):\n",
    "            if reset_opt or not self.opt: self.create_opt()\n",
    "            self.opt.set_hypers(wd=wd, lr=self.lr if lr is None else lr)\n",
    "\n",
    "            try:\n",
    "                self._do_begin_fit(n_epoch)\n",
    "                for epoch in range(n_epoch):\n",
    "                    try:\n",
    "                        self.epoch=epoch;          self('begin_epoch')\n",
    "                        self._do_epoch_train()\n",
    "                        self._do_epoch_validate()\n",
    "                    except CancelEpochException:   self('after_cancel_epoch')\n",
    "                    finally:                       self('after_epoch')\n",
    "\n",
    "            except CancelFitException:             self('after_cancel_fit')\n",
    "            finally:                               self('after_fit')\n",
    "\n",
    "    def validate(self, ds_idx=1, dl=None, cbs=None):\n",
    "        self.epoch,self.n_epoch,self.loss = 0,1,tensor(0.)\n",
    "        self.dl = self.dbunch.dls[ds_idx] if dl is None else dl\n",
    "        with self.added_cbs(cbs), self.no_logging():\n",
    "            self(_before_epoch)\n",
    "            self._do_epoch_validate(ds_idx, dl)\n",
    "            self(_after_epoch)\n",
    "        return self.recorder.values[-1]\n",
    "\n",
    "    def get_preds(self, ds_idx=1, dl=None, with_input=False, with_loss=False, decoded=False, act=None):\n",
    "        self.epoch,self.n_epoch,self.loss = 0,1,tensor(0.)\n",
    "        cb = GatherPredsCallback(with_input=with_input, with_loss=with_loss)\n",
    "        with self.no_logging(), self.added_cbs(cb), self.loss_not_reduced():\n",
    "            self(_before_epoch)\n",
    "            self._do_epoch_validate(ds_idx, dl)\n",
    "            self(_after_epoch)\n",
    "            if act is None: act = getattr(self.loss_func, 'activation', noop)\n",
    "            preds = act(torch.cat(cb.preds))\n",
    "            if decoded: preds = getattr(sellf.loss_func, 'decodes', noop)(preds)\n",
    "            res = (preds, detuplify(tuple(torch.cat(o) for o in zip(*cb.targets))))\n",
    "            if with_input: res = (tuple(torch.cat(o) for o in zip(*cb.inputs)),) + res\n",
    "            if with_loss:  res = res + (torch.cat(cb.losses),)\n",
    "            return res\n",
    "        \n",
    "    def predict(self, item):\n",
    "        dl = test_dl(self.dbunch, [item])\n",
    "        inp,preds,_ = self.get_preds(dl=dl, with_input=True)\n",
    "        dec_preds = getattr(self.loss_func, 'decodes', noop)(preds)\n",
    "        i = getattr(self.dbunch, 'n_inp', -1)\n",
    "        full_dec = self.dbunch.decode_batch((*inp,dec_preds))[0][i:]\n",
    "        return detuplify(full_dec),dec_preds[0],preds[0]\n",
    "    \n",
    "    def show_results(self, ds_idx=0, dl=None, max_n=10, **kwargs):\n",
    "        dl = self.dbunch.dls[ds_idx] if dl is None else dl\n",
    "        b = dl.one_batch()\n",
    "        preds,_ = self.get_preds(dl=[b])\n",
    "        preds = getattr(self.loss_func, \"decodes\", noop)(preds)\n",
    "        self.dbunch.show_results(b, preds, max_n=max_n, **kwargs)\n",
    "\n",
    "    @contextmanager\n",
    "    def no_logging(self): return replacing_yield(self, 'logger', noop)\n",
    "\n",
    "    @contextmanager\n",
    "    def loss_not_reduced(self):\n",
    "        if hasattr(self.loss_func, 'reduction'): return replacing_yield(self.loss_func, 'reduction', 'none')\n",
    "        else: return replacing_yield(self, 'loss_func', partial(self.loss_func, reduction='none'))\n",
    "            \n",
    "    def save(self, file, with_opt=True):\n",
    "        #TODO: if rank_distrib(): return # don't save if slave proc\n",
    "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        save_model(file, self.model, getattr(self,'opt',None), with_opt)\n",
    "\n",
    "    def load(self, file, with_opt=None, device=None, strict=True):\n",
    "        if device is None: device = self.dbunch.device\n",
    "        if self.opt is None: self.create_opt()\n",
    "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        load_model(file, self.model, self.opt, with_opt=with_opt, device=device, strict=strict)\n",
    "        return self\n",
    "\n",
    "Learner.x,Learner.y = add_props(lambda i,x: detuplify((x.xb,x.yb)[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "add_docs(Learner, \"Group together a `model`, some `dbunch` and a `loss_func` to handle training\",\n",
    "    add_cbs=\"Add `cbs` to the list of `Callback` and register `self` as their learner\",\n",
    "    add_cb=\"Add `cb` to the list of `Callback` and register `self` as their learner\",\n",
    "    remove_cbs=\"Remove `cbs` from the list of `Callback` and deregister `self` as their learner\",\n",
    "    remove_cb=\"Add `cb` from the list of `Callback` and deregister `self` as their learner\",\n",
    "    added_cbs=\"Context manage that temporarily adds `cbs`\",\n",
    "    create_opt=\"Create an optimizer with `lr`\",\n",
    "    one_batch=\"Train or evaluate `self.model` on batch `(xb,yb)`\",\n",
    "    all_batches=\"Train or evaluate `self.model` on all batches of `self.dl`\",\n",
    "    fit=\"Fit `self.model` for `n_epoch` using `cbs`. Optionally `reset_opt`.\",\n",
    "    validate=\"Validate on `dl` with potential new `cbs`.\",\n",
    "    get_preds=\"Get the predictions and targets on the `ds_idx`-th dbunchset or `dl`, optionally `with_input` and `with_loss`\",\n",
    "    predict=\"Return the prediction on `item`, fully decoded, loss function decoded and probabilities\",\n",
    "    show_results=\"Show some predictions on `ds_idx`-th dbunchset or `dl`\",\n",
    "    no_logging=\"Context manager to temporarily remove `logger`\",\n",
    "    loss_not_reduced=\"A context manager to evaluate `loss_func` with reduction set to none.\",\n",
    "    save=\"Save model and optimizer state (if `with_opt`) to `self.path/self.model_dir/file`\",\n",
    "    load=\"Load model and optimizer state (if `with_opt`) from `self.path/self.model_dir/file` using `device`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`opt_func` will be used to create an optimizer when `Learner.fit` is called, with `lr` as a learning rate. `splitter` is a function taht takes `self.model` and returns a list of parameter groups (or just one parameter group if there are no different parameter groups). The default is `trainable_params`, which returns all trainable parameters of the model.\n",
    "\n",
    "`cbs` is one or a list of `Callback`s  to pass to the `Learner`, and `cb_funcs` is one or a list of functions returning a `Callback` that will be called at init. Each `Callback` is registered as an attribute of `Learner` (with camel case). At creation, all the callbacks in `defaults.callbacks` (`TrainEvalCallback` and `Recorder`) are associated to the `Learner`.\n",
    "\n",
    "`metrics` is an optional list of metrics, that can be either functions or `Metric`s (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test init with callbacks\n",
    "def synth_learner(n_train=10, n_valid=2, cuda=False, lr=defaults.lr, **kwargs):\n",
    "    data = synth_dbunch(n_train=n_train,n_valid=n_valid, cuda=cuda)\n",
    "    return Learner(data, RegModel(), loss_func=MSELossFlat(), lr=lr, **kwargs)\n",
    "\n",
    "tst_learn = synth_learner()\n",
    "test_eq(len(tst_learn.cbs), 1)\n",
    "assert isinstance(tst_learn.cbs[0], TrainEvalCallback)\n",
    "assert hasattr(tst_learn, ('train_eval'))\n",
    "\n",
    "tst_learn = synth_learner(cbs=TstCallback())\n",
    "test_eq(len(tst_learn.cbs), 2)\n",
    "assert isinstance(tst_learn.cbs[1], TstCallback)\n",
    "assert hasattr(tst_learn, ('tst'))\n",
    "\n",
    "tst_learn = synth_learner(cb_funcs=TstCallback)\n",
    "test_eq(len(tst_learn.cbs), 2)\n",
    "assert isinstance(tst_learn.cbs[1], TstCallback)\n",
    "assert hasattr(tst_learn, ('tst'))\n",
    "\n",
    "#A name that becomes an existing attribute of the Learner will throw an exception (here add_cb)\n",
    "class AddCbCallback(Callback): pass\n",
    "test_fail(lambda: synth_learner(cbs=AddCbCallback()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.fit\" class=\"doc_header\"><code>Learner.fit</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L92\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.fit</code>(**`n_epoch`**, **`lr`**=*`None`*, **`wd`**=*`0.01`*, **`cbs`**=*`None`*, **`reset_opt`**=*`False`*)\n",
       "\n",
       "Fit `self.model` for `n_epoch` using `cbs`. Optionally `reset_opt`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a few epochs should make the model better\n",
    "learn = synth_learner(cb_funcs=TstCallback, lr=1e-2)\n",
    "xb,yb = learn.dbunch.one_batch()\n",
    "init_loss = learn.loss_func(learn.model(xb), yb)\n",
    "learn.fit(2)\n",
    "assert learn.loss < init_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test of TrainEvalCallback\n",
    "class TestTrainEvalCallback(Callback):\n",
    "    run_after=TrainEvalCallback\n",
    "    def begin_fit(self): \n",
    "        test_eq([self.pct_train,self.train_iter], [0., 0])\n",
    "        self.old_pct_train,self.old_train_iter = self.pct_train,self.train_iter\n",
    "    \n",
    "    def begin_batch(self): test_eq(next(self.model.parameters()).device, find_device(self.xb))\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if self.training:\n",
    "            test_eq(self.pct_train , self.old_pct_train+1/(self.n_iter*self.n_epoch))\n",
    "            test_eq(self.train_iter, self.old_train_iter+1)\n",
    "            self.old_pct_train,self.old_train_iter = self.pct_train,self.train_iter\n",
    "    \n",
    "    def begin_train(self):\n",
    "        assert self.training and self.model.training\n",
    "        test_eq(self.pct_train, self.epoch/self.n_epoch)\n",
    "        self.old_pct_train = self.pct_train\n",
    "    \n",
    "    def begin_validate(self):\n",
    "        assert not self.training and not self.model.training\n",
    "        \n",
    "learn = synth_learner(cb_funcs=TestTrainEvalCallback)\n",
    "learn.fit(1)\n",
    "#Check order is properly taken into account\n",
    "learn.cbs = L(reversed(learn.cbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#cuda\n",
    "#Check model is put on the GPU if needed\n",
    "learn = synth_learner(cb_funcs=TestTrainEvalCallback, cuda=True)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Check wd is not applied on bn/bias when option wd_bn_bias=False\n",
    "class _TstModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n",
    "        self.tst = nn.Sequential(nn.Linear(4,5), nn.BatchNorm1d(3))\n",
    "        self.tst[0].bias.data,self.tst[1].bias.data = torch.randn(5),torch.randn(3) \n",
    "    def forward(self, x): return x * self.a + self.b\n",
    "    \n",
    "class _PutGrad(Callback):\n",
    "    def after_backward(self):\n",
    "        for p in self.learn.model.tst.parameters():\n",
    "            p.grad = torch.ones_like(p.data)\n",
    "    \n",
    "learn = synth_learner(n_train=5, opt_func = partial(SGD, wd=1, true_wd=True), cb_funcs=_PutGrad)\n",
    "learn.model = _TstModel()\n",
    "init = [p.clone() for p in learn.model.tst.parameters()]\n",
    "learn.fit(1, lr=1e-2)\n",
    "end = list(learn.model.tst.parameters())\n",
    "assert not torch.allclose(end[0]-init[0], -0.05 * torch.ones_like(end[0]))\n",
    "for i in [1,2,3]: test_close(end[i]-init[i], -0.05 * torch.ones_like(end[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.one_batch\" class=\"doc_header\"><code>Learner.one_batch</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.one_batch</code>(**`i`**, **`b`**)\n",
       "\n",
       "Train or evaluate `self.model` on batch `(xb,yb)`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.one_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an internal method called by `Learner.fit`. If passed, `i` is the index of this iteration in the epoch. In training method, this does a full training step on the batch (compute predictions, loss, gradients, update the model parameters and zero the gradients). In validation mode, it stops at the loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class VerboseCallback(Callback):\n",
    "    \"Callback that prints the name of each event called\"\n",
    "    def __call__(self, event_name):\n",
    "        print(event_name)\n",
    "        super().__call__(event_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class TestOneBatch(VerboseCallback):\n",
    "    def __init__(self, xb, yb, i):\n",
    "        self.save_xb,self.save_yb,self.i = xb,yb,i\n",
    "        self.old_pred,self.old_loss = None,tensor(0.)\n",
    "        \n",
    "    def begin_batch(self):\n",
    "        self.old_a,self.old_b = self.model.a.data.clone(),self.model.b.data.clone()\n",
    "        test_eq(self.iter,    self.i)\n",
    "        test_eq(self.save_xb, *self.xb)\n",
    "        test_eq(self.save_yb, *self.yb)\n",
    "        if hasattr(self.learn, 'pred'): test_eq(self.pred, self.old_pred)\n",
    "    \n",
    "    def after_pred(self):\n",
    "        self.old_pred = self.pred\n",
    "        test_eq(self.pred, self.model.a.data * self.x + self.model.b.data)\n",
    "        test_eq(self.loss, self.old_loss)\n",
    "    \n",
    "    def after_loss(self):\n",
    "        self.old_loss = self.loss\n",
    "        test_eq(self.loss, self.loss_func(self.old_pred, self.save_yb))\n",
    "        for p in self.model.parameters(): \n",
    "            if not hasattr(p, 'grad') or p.grad is not None: test_eq(p.grad, tensor([0.]))\n",
    "    \n",
    "    def after_backward(self):\n",
    "        self.grad_a = (2 * self.x * (self.pred.data - self.y)).mean()\n",
    "        self.grad_b = 2 * (self.pred.data - self.y).mean()\n",
    "        test_close(self.model.a.grad.data, self.grad_a)\n",
    "        test_close(self.model.b.grad.data, self.grad_b)\n",
    "        test_eq(self.model.a.data, self.old_a)\n",
    "        test_eq(self.model.b.data, self.old_b)\n",
    "        \n",
    "    def after_step(self):\n",
    "        test_close(self.model.a.data, self.old_a - self.lr * self.grad_a)\n",
    "        test_close(self.model.b.data, self.old_b - self.lr * self.grad_b)\n",
    "        self.old_a,self.old_b = self.model.a.data.clone(),self.model.b.data.clone()\n",
    "        test_close(self.model.a.grad.data, self.grad_a)\n",
    "        test_close(self.model.b.grad.data, self.grad_b)\n",
    "    \n",
    "    def after_batch(self):\n",
    "        for p in self.model.parameters(): test_eq(p.grad, tensor([0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "learn = synth_learner()\n",
    "b = learn.dbunch.one_batch()\n",
    "learn = synth_learner(cbs=TestOneBatch(*b, 42), lr=1e-2)\n",
    "#Remove train/eval\n",
    "learn.cbs = learn.cbs[1:]\n",
    "#Setup\n",
    "learn.loss,learn.training = tensor(0.),True\n",
    "learn.opt = SGD(learn.model.parameters(), lr=learn.lr)\n",
    "learn.model.train()\n",
    "batch_events = ['begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step', 'after_batch']\n",
    "test_stdout(lambda: learn.one_batch(42, b), '\\n'.join(batch_events))\n",
    "test_stdout(lambda: learn.one_batch(42, b), '\\n'.join(batch_events)) #Check it works for a second batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.all_batches\" class=\"doc_header\"><code>Learner.all_batches</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L56\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.all_batches</code>()\n",
       "\n",
       "Train or evaluate `self.model` on all batches of `self.dl`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.all_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "learn = synth_learner(n_train=5, cbs=VerboseCallback())\n",
    "learn.opt = SGD(learn.model.parameters(), lr=learn.lr)\n",
    "with redirect_stdout(io.StringIO()): \n",
    "    learn._do_begin_fit(1)\n",
    "    learn.epoch,learn.dl = 0,learn.dbunch.train_dl\n",
    "    learn('begin_epoch')\n",
    "    learn('begin_train')\n",
    "test_stdout(learn.all_batches, '\\n'.join(batch_events * 5))\n",
    "test_eq(learn.train_iter, 5)\n",
    "\n",
    "valid_events = ['begin_batch', 'after_pred', 'after_loss', 'after_batch']\n",
    "with redirect_stdout(io.StringIO()): \n",
    "    learn.dl = learn.dbunch.valid_dl\n",
    "    learn('begin_validate')\n",
    "test_stdout(learn.all_batches, '\\n'.join(valid_events * 2))\n",
    "test_eq(learn.train_iter, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "learn = synth_learner(n_train=5, cbs=VerboseCallback())\n",
    "test_stdout(lambda: learn._do_begin_fit(42), 'begin_fit')\n",
    "test_eq(learn.n_epoch, 42)\n",
    "test_eq(learn.loss, tensor(0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "learn.opt = SGD(learn.model.parameters(), lr=learn.lr)\n",
    "learn.epoch = 0\n",
    "test_stdout(lambda: learn._do_epoch_train(), '\\n'.join(['begin_train'] + batch_events * 5 + ['after_train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_stdout(learn._do_epoch_validate, '\\n'.join(['begin_validate'] + valid_events * 2+ ['after_validate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.save\" class=\"doc_header\"><code>Learner.save</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L157\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.save</code>(**`file`**, **`with_opt`**=*`True`*)\n",
       "\n",
       "Save model and optimizer state (if `with_opt`) to `self.path/self.model_dir/file`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file` can be a `Path`, a `string` or a buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.load\" class=\"doc_header\"><code>Learner.load</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L162\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.load</code>(**`file`**, **`with_opt`**=*`None`*, **`device`**=*`None`*, **`strict`**=*`True`*)\n",
       "\n",
       "Load model and optimizer state (if `with_opt`) from `self.path/self.model_dir/file` using [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file` can be a `Path`, a `string` or a buffer. Use `device` to load the model/optimizer state on a device different from the one it was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = synth_learner(cb_funcs=TstCallback, opt_func=partial(SGD, mom=0.9))\n",
    "xb,yb = learn.dbunch.one_batch()\n",
    "init_loss = learn.loss_func(learn.model(xb), yb)\n",
    "learn.fit(1)\n",
    "learn.save('tmp')\n",
    "assert (Path.cwd()/'models/tmp.pth').exists()\n",
    "\n",
    "learn1 = synth_learner(cb_funcs=TstCallback, opt_func=partial(SGD, mom=0.9))\n",
    "learn1 = learn1.load('tmp')\n",
    "test_eq(learn.model.a, learn1.model.a)\n",
    "test_eq(learn.model.b, learn1.model.b)\n",
    "test_eq(learn.opt.state_dict(), learn1.opt.state_dict())\n",
    "\n",
    "learn.save('tmp1', with_opt=False)\n",
    "learn1 = synth_learner(cb_funcs=TstCallback, opt_func=partial(SGD, mom=0.9))\n",
    "learn1 = learn1.load('tmp1')\n",
    "test_eq(learn.model.a, learn1.model.a)\n",
    "test_eq(learn.model.b, learn1.model.b)\n",
    "test_ne(learn.opt.state_dict(), learn1.opt.state_dict())\n",
    "\n",
    "shutil.rmtree('models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.__call__\" class=\"doc_header\"><code>Learner.__call__</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.__call__</code>(**`event_name`**)\n",
       "\n",
       "Call self as a function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.add_cb\" class=\"doc_header\"><code>Learner.add_cb</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.add_cb</code>(**`cb`**)\n",
       "\n",
       "Add `cb` to the list of [`Callback`](/learner.html#Callback) and register `self` as their learner"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.add_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = synth_learner()\n",
    "learn.add_cb(TestTrainEvalCallback())\n",
    "test_eq(len(learn.cbs), 2)\n",
    "assert isinstance(learn.cbs[1], TestTrainEvalCallback)\n",
    "test_eq(learn.train_eval.learn, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.add_cbs\" class=\"doc_header\"><code>Learner.add_cbs</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.add_cbs</code>(**`cbs`**)\n",
       "\n",
       "Add `cbs` to the list of [`Callback`](/learner.html#Callback) and register `self` as their learner"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.add_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.add_cbs([TestTrainEvalCallback(), TestTrainEvalCallback()])\n",
    "test_eq(len(learn.cbs), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.remove_cb\" class=\"doc_header\"><code>Learner.remove_cb</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L28\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.remove_cb</code>(**`cb`**)\n",
       "\n",
       "Add `cb` from the list of [`Callback`](/learner.html#Callback) and deregister `self` as their learner"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.remove_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = learn.cbs[1]\n",
    "learn.remove_cb(learn.cbs[1])\n",
    "test_eq(len(learn.cbs), 3)\n",
    "assert cb.learn is None\n",
    "assert not getattr(learn,'test_train_eval',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.remove_cbs\" class=\"doc_header\"><code>Learner.remove_cbs</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.remove_cbs</code>(**`cbs`**)\n",
       "\n",
       "Remove `cbs` from the list of [`Callback`](/learner.html#Callback) and deregister `self` as their learner"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.remove_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = learn.cbs[1]\n",
    "learn.remove_cbs(learn.cbs[1:])\n",
    "test_eq(len(learn.cbs), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing a callback, the following attributes of `Learner` are available:\n",
    "- `model`: the model used for training/validation\n",
    "- `data`: the underlying `DataBunch`\n",
    "- `loss_func`: the loss function used\n",
    "- `opt`: the optimizer used to udpate the model parameters\n",
    "- `opt_func`: the function used to create the optimizer\n",
    "- `cbs`: the list containing all `Callback`s\n",
    "- `dl`: current `DataLoader` used for iteration\n",
    "- `x`/`xb`: last input drawn from `self.dl` (potentially modified by callbacks). `xb` is always a tuple (potentially with one element) and `x` is detuplified. You can only assign to `xb`.\n",
    "- `y`/`yb`: last target drawn from `self.dl` (potentially modified by callbacks). `yb` is always a tuple (potentially with one element) and `y` is detuplified. You can only assign to `yb`.\n",
    "- `pred`: last predictions from `self.model` (potentially modified by callbacks)\n",
    "- `loss`: last computed loss (potentially modified by callbacks)\n",
    "- `n_epoch`: the number of epochs in this training\n",
    "- `n_iter`: the number of iterations in the current `self.dl`\n",
    "- `epoch`: the current epoch index (from 0 to `n_epoch-1`)\n",
    "- `iter`: the current iteration index in `self.dl` (from 0 to `n_iter-1`)\n",
    "\n",
    "The following attributes are added by `TrainEvalCallback` and should be available unless you went out of your way to remove that callback:\n",
    "\n",
    "- `train_iter`: the number of training iterations done since the beginning of this training\n",
    "- `pct_train`: from 0. to 1., the percentage of training iterations completed\n",
    "- `training`:  flag to indicate if we're in training mode or not\n",
    "\n",
    "The following attribute is added by `Recorder` and should be available unless you went out of your way to remove that callback:\n",
    "\n",
    "- `smooth_loss`: an exponentially-averaged version of the training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control flow testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "batch_events  = ['begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step', 'after_batch']\n",
    "batchv_events = ['begin_batch', 'after_pred', 'after_loss', 'after_batch']\n",
    "train_events  = ['begin_train']    + batch_events  + ['after_train']\n",
    "valid_events  = ['begin_validate'] + batchv_events + ['after_validate']\n",
    "epoch_events  = ['begin_epoch'] + train_events + valid_events + ['after_epoch']\n",
    "cycle_events  = ['begin_fit'] + epoch_events + ['after_fit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "learn = synth_learner(n_train=1, n_valid=1)\n",
    "test_stdout(lambda: learn.fit(1, cbs=VerboseCallback()), '\\n'.join(cycle_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class TestCancelCallback(VerboseCallback):\n",
    "    def __init__(self, cancel_at=event.begin_batch, exception=CancelBatchException, train=None):\n",
    "        def _interrupt(): \n",
    "            if train is None or train == self.training: raise exception()\n",
    "        setattr(self, cancel_at, _interrupt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test cancel batch\n",
    "for i,e in enumerate(batch_events[:-1]):\n",
    "    be = batch_events[:i+1] + ['after_cancel_batch', 'after_batch']\n",
    "    bev = be if i <3 else batchv_events\n",
    "    cycle = cycle_events[:3] + be + ['after_train', 'begin_validate'] + bev + cycle_events[-3:]\n",
    "    test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback(cancel_at=e)), '\\n'.join(cycle))\n",
    "\n",
    "#CancelBatchException not caught if thrown in any other event\n",
    "for e in cycle_events:\n",
    "    if e not in batch_events[:-1]:\n",
    "        with redirect_stdout(io.StringIO()):\n",
    "            cb = TestCancelCallback(cancel_at=e)\n",
    "            test_fail(lambda: learn.fit(1, cbs=cb))\n",
    "            learn.remove_cb(cb) #Have to remove it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test cancel train\n",
    "for i,e in enumerate(['begin_train'] + batch_events):\n",
    "    be = batch_events[:i] + (['after_batch'] if i >=1 and i < len(batch_events) else []) \n",
    "    be += ['after_cancel_train', 'after_train']\n",
    "    cycle = cycle_events[:3] + be + ['begin_validate'] + batchv_events + cycle_events[-3:]\n",
    "    test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback(e, CancelTrainException, True)), '\\n'.join(cycle))\n",
    "\n",
    "#CancelTrainException not caught if thrown in any other event\n",
    "for e in cycle_events:\n",
    "    if e not in ['begin_train'] + batch_events[:-1]:\n",
    "        with redirect_stdout(io.StringIO()):\n",
    "            cb = TestCancelCallback(e, CancelTrainException)\n",
    "            test_fail(lambda: learn.fit(1, cbs=cb))\n",
    "            learn.remove_cb(cb) #Have to remove it manually  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test cancel valid\n",
    "for i,e in enumerate(['begin_validate'] + batchv_events):\n",
    "    bev = batchv_events[:i] + (['after_batch'] if i >=1 and i < len(batchv_events) else []) + ['after_cancel_validate']\n",
    "    cycle = cycle_events[:3] + batch_events + ['after_train', 'begin_validate'] + bev + cycle_events[-3:]\n",
    "    test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback(e, CancelValidException, False)), '\\n'.join(cycle))\n",
    "    \n",
    "#CancelValidException not caught if thrown in any other event\n",
    "for e in cycle_events:\n",
    "    if e not in ['begin_validate'] + batch_events[:3]:\n",
    "        with redirect_stdout(io.StringIO()):\n",
    "            cb = TestCancelCallback(e, CancelValidException)\n",
    "            test_fail(lambda: learn.fit(1, cbs=cb))\n",
    "            learn.remove_cb(cb) #Have to remove it manually  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test cancel epoch\n",
    "#In train\n",
    "for i,e in enumerate(['begin_train'] + batch_events):\n",
    "    be = batch_events[:i] + (['after_batch'] if i >=1 and i<len(batch_events) else []) \n",
    "    cycle = cycle_events[:3] + be + ['after_train', 'after_cancel_epoch'] + cycle_events[-2:]\n",
    "    test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback(e, CancelEpochException, True)), '\\n'.join(cycle))\n",
    "\n",
    "#In valid\n",
    "for i,e in enumerate(['begin_validate'] + batchv_events):\n",
    "    bev = batchv_events[:i] + (['after_batch'] if i >=1 and i<len(batchv_events) else [])\n",
    "    cycle = cycle_events[:3] + batch_events + ['after_train', 'begin_validate'] + bev \n",
    "    cycle += ['after_validate', 'after_cancel_epoch'] + cycle_events[-2:]\n",
    "    test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback(e, CancelEpochException, False)), '\\n'.join(cycle))\n",
    "\n",
    "#In begin epoch\n",
    "test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback('begin_epoch', CancelEpochException, False)), \n",
    "            '\\n'.join(cycle_events[:2] + ['after_cancel_epoch'] + cycle_events[-2:]))\n",
    "\n",
    "#CancelEpochException not caught if thrown in any other event\n",
    "for e in ['begin_fit', 'after_epoch', 'after_fit']:\n",
    "    if e not in ['begin_validate'] + batch_events[:3]:\n",
    "        with redirect_stdout(io.StringIO()):\n",
    "            cb = TestCancelCallback(e, CancelEpochException)\n",
    "            test_fail(lambda: learn.fit(1, cbs=cb))\n",
    "            learn.remove_cb(cb) #Have to remove it manually  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#test cancel fit\n",
    "#In begin fit\n",
    "test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback('begin_fit', CancelFitException)), \n",
    "            '\\n'.join(['begin_fit', 'after_cancel_fit', 'after_fit']))\n",
    "\n",
    "#In begin epoch\n",
    "test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback('begin_epoch', CancelFitException, False)), \n",
    "            '\\n'.join(cycle_events[:2] + ['after_epoch', 'after_cancel_fit', 'after_fit']))\n",
    "#In train\n",
    "for i,e in enumerate(['begin_train'] + batch_events):\n",
    "    be = batch_events[:i] + (['after_batch'] if i >=1 and i<len(batch_events) else []) \n",
    "    cycle = cycle_events[:3] + be + ['after_train', 'after_epoch', 'after_cancel_fit', 'after_fit']\n",
    "    test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback(e, CancelFitException, True)), '\\n'.join(cycle))\n",
    "    \n",
    "#In valid\n",
    "for i,e in enumerate(['begin_validate'] + batchv_events):\n",
    "    bev = batchv_events[:i] + (['after_batch'] if i >=1 and i<len(batchv_events) else [])\n",
    "    cycle = cycle_events[:3] + batch_events + ['after_train', 'begin_validate'] + bev \n",
    "    cycle += ['after_validate', 'after_epoch', 'after_cancel_fit', 'after_fit']\n",
    "    test_stdout(lambda: learn.fit(1, cbs=TestCancelCallback(e, CancelFitException, False)), '\\n'.join(cycle))\n",
    "    \n",
    "#CancelEpochException not caught if thrown in any other event\n",
    "with redirect_stdout(io.StringIO()):\n",
    "    cb = TestCancelCallback('after_fit', CancelEpochException)\n",
    "    test_fail(lambda: learn.fit(1, cbs=cb))\n",
    "    learn.remove_cb(cb) #Have to remove it manually  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@docs\n",
    "class Metric():\n",
    "    \"Blueprint for defining a metric\"\n",
    "    def reset(self): pass\n",
    "    def accumulate(self, learn): pass\n",
    "    @property\n",
    "    def value(self): raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def name(self): return class2attr(self, 'Metric')\n",
    "\n",
    "    _docs = dict(\n",
    "        reset=\"Reset inner state to prepare for new computation\",\n",
    "        name=\"Name of the `Metric`, camel-cased and with Metric removed\",\n",
    "        accumulate=\"Use `learn` to update the state with new results\",\n",
    "        value=\"The value of the metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"Metric\" class=\"doc_header\"><code>class</code> <code>Metric</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>Metric</code>()\n",
       "\n",
       "Blueprint for defining a metric"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Metric, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics can be simple averages (like accuracy) but sometimes their computation is a little bit more complex and can't be averaged over batches (like precision or recall), which is why we need a special class for them. For simple functions that can be computed as averages over batches, we can use the class `AvgMetric`, otherwise you'll need to implement the following methods.\n",
    "\n",
    "> Note: If your `Metric` has state depending on tensors, don't forget to store it on the CPU to avoid any potential memory leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Metric.reset\" class=\"doc_header\"><code>Metric.reset</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Metric.reset</code>()\n",
       "\n",
       "Reset inner state to prepare for new computation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Metric.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Metric.accumulate\" class=\"doc_header\"><code>Metric.accumulate</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L6\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Metric.accumulate</code>(**`learn`**)\n",
       "\n",
       "Use `learn` to update the state with new results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Metric.accumulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Metric.value\" class=\"doc_header\"><code>Metric.value</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "The value of the metric"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Metric.value, name='Metric.value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Metric.name\" class=\"doc_header\"><code>Metric.name</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Name of the [`Metric`](/learner.html#Metric), camel-cased and with Metric removed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Metric.name, name='Metric.name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgMetric(Metric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func):  self.func = func\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += to_detach(self.func(learn.pred, *learn.yb))*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return self.func.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"AvgMetric\" class=\"doc_header\"><code>class</code> <code>AvgMetric</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>AvgMetric</code>(**`func`**) :: [`Metric`](/learner.html#Metric)\n",
       "\n",
       "Average the values of `func` taking into account potential different batch sizes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AvgMetric, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = synth_learner()\n",
    "tst = AvgMetric(lambda x,y: (x-y).abs().mean())\n",
    "t,u = torch.randn(100),torch.randn(100)\n",
    "tst.reset()\n",
    "for i in range(0,100,25): \n",
    "    learn.pred,learn.yb = t[i:i+25],(u[i:i+25],)\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, (t-u).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#With varying batch size\n",
    "tst.reset()\n",
    "splits = [0, 30, 50, 60, 100]\n",
    "for i in range(len(splits )-1): \n",
    "    learn.pred,learn.yb = t[splits[i]:splits[i+1]],(u[splits[i]:splits[i+1]],)\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, (t-u).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgLoss(Metric):\n",
    "    \"Average the losses taking into account potential different batch sizes\"\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += to_detach(learn.loss.mean())*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"AvgLoss\" class=\"doc_header\"><code>class</code> <code>AvgLoss</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>AvgLoss</code>() :: [`Metric`](/learner.html#Metric)\n",
       "\n",
       "Average the losses taking into account potential different batch sizes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AvgLoss, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = AvgLoss()\n",
    "t = torch.randn(100)\n",
    "tst.reset()\n",
    "for i in range(0,100,25): \n",
    "    learn.yb,learn.loss = t[i:i+25],t[i:i+25].mean()\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#With varying batch size\n",
    "tst.reset()\n",
    "splits = [0, 30, 50, 60, 100]\n",
    "for i in range(len(splits )-1): \n",
    "    learn.yb,learn.loss = t[splits[i]:splits[i+1]],t[splits[i]:splits[i+1]].mean()\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgSmoothLoss(Metric):\n",
    "    \"Smooth average of the losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, beta=0.98): self.beta = beta\n",
    "    def reset(self):               self.count,self.val = 0,tensor(0.)\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.loss.mean()), self.val, self.beta)\n",
    "    @property\n",
    "    def value(self): return self.val/(1-self.beta**self.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"AvgSmoothLoss\" class=\"doc_header\"><code>class</code> <code>AvgSmoothLoss</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>AvgSmoothLoss</code>(**`beta`**=*`0.98`*) :: [`Metric`](/learner.html#Metric)\n",
       "\n",
       "Smooth average of the losses (exponentially weighted with `beta`)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AvgSmoothLoss, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = AvgSmoothLoss()\n",
    "t = torch.randn(100)\n",
    "tst.reset()\n",
    "val = tensor(0.)\n",
    "for i in range(4): \n",
    "    learn.loss = t[i*25:(i+1)*25].mean()\n",
    "    tst.accumulate(learn)\n",
    "    val = val*0.98 + t[i*25:(i+1)*25].mean()*(1-0.98)\n",
    "    test_close(val/(1-0.98**(i+1)), tst.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recorder --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastprogress.fastprogress import format_time\n",
    "\n",
    "def _maybe_item(t):\n",
    "    t = t.value\n",
    "    return t.item() if isinstance(t, Tensor) and t.numel()==1 else t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Recorder(Callback):\n",
    "    \"Callback that registers statistics (lr, loss and metrics) during training\"\n",
    "    run_after = TrainEvalCallback\n",
    "\n",
    "    def __init__(self, add_time=True, train_metrics=False, beta=0.98):\n",
    "        self.add_time,self.train_metrics = add_time,train_metrics\n",
    "        self.loss,self.smooth_loss = AvgLoss(),AvgSmoothLoss(beta=beta)\n",
    "\n",
    "    def begin_fit(self):\n",
    "        \"Prepare state for training\"\n",
    "        self.lrs,self.losses,self.values = [],[],[]\n",
    "        names = self._valid_mets.attrgot('name')\n",
    "        if self.train_metrics: names = names.map('train_{}') + names.map('valid_{}')\n",
    "        else:                  names = L('train_loss', 'valid_loss') + names[1:]\n",
    "        if self.add_time: names.append('time')\n",
    "        self.metric_names = 'epoch'+names\n",
    "        self.smooth_loss.reset()\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Update all metrics and records lr and smooth loss in training\"\n",
    "        if len(self.yb) == 0: return\n",
    "        mets = L(self.smooth_loss) + (self._train_mets if self.training else self._valid_mets)\n",
    "        for met in mets: met.accumulate(self.learn)\n",
    "        if not self.training: return\n",
    "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "        self.losses.append(self.smooth_loss.value)\n",
    "        self.learn.smooth_loss = self.smooth_loss.value\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        \"Set timer if `self.add_time=True`\"\n",
    "        self.cancel_train,self.cancel_valid = False,False\n",
    "        if self.add_time: self.start_epoch = time.time()\n",
    "        self.log = L(getattr(self, 'epoch', 0))\n",
    "\n",
    "    def begin_train   (self): self._train_mets.map(Self.reset())\n",
    "    def begin_validate(self): self._valid_mets.map(Self.reset())\n",
    "    def after_train   (self): self.log += self._train_mets.map(_maybe_item)\n",
    "    def after_validate(self): self.log += self._valid_mets.map(_maybe_item)\n",
    "    def after_cancel_train(self):    self.cancel_train = True\n",
    "    def after_cancel_validate(self): self.cancel_valid = True\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Store and log the loss/metric values\"\n",
    "        self.values.append(self.log[1:].copy())\n",
    "        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n",
    "        self.logger(self.log)\n",
    "\n",
    "    @property\n",
    "    def _train_mets(self):\n",
    "        if getattr(self, 'cancel_train', False): return L()\n",
    "        return L(self.loss) + (self.metrics if self.train_metrics else L())\n",
    "    \n",
    "    @property\n",
    "    def _valid_mets(self):\n",
    "        if getattr(self, 'cancel_valid', False): return L()\n",
    "        return L(self.loss) + self.metrics\n",
    "\n",
    "    def plot_loss(self, skip_start=5): plt.plot(self.losses[skip_start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "add_docs(Recorder,\n",
    "         begin_train = \"Reset loss and metrics state\",\n",
    "         after_train = \"Log loss and metric values on the training set (if `self.training_metrics=True`)\",\n",
    "         begin_validate = \"Reset loss and metrics state\",\n",
    "         after_validate = \"Log loss and metric values on the validation set\",\n",
    "         after_cancel_train = \"Ignore training metrics for this epoch\",\n",
    "         after_cancel_validate = \"Ignore validation metrics for this epoch\",\n",
    "         plot_loss = \"Plot the losses from `skip_start` and onward\")\n",
    "\n",
    "defaults.callbacks = [TrainEvalCallback, Recorder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, metrics are computed on the validation set only, although that can be changed with `training_metrics=True`. `beta` is the weight used to compute the exponentially weighted average of the losses (which gives the `smooth_loss` attribute to `Learner`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test printed output\n",
    "def tst_metric(out, targ): return F.mse_loss(out, targ)\n",
    "learn = synth_learner(n_train=5, metrics=tst_metric)\n",
    "pat = r\"[tensor\\(\\d.\\d*\\), tensor\\(\\d.\\d*\\), tensor\\(\\d.\\d*\\), 'dd:dd']\"\n",
    "test_stdout(lambda: learn.fit(1), pat, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class TestRecorderCallback(Callback):\n",
    "    run_after=Recorder\n",
    "    \n",
    "    def begin_fit(self): \n",
    "        self.train_metrics,self.add_time = self.recorder.train_metrics,self.recorder.add_time\n",
    "        self.beta = self.recorder.smooth_loss.beta\n",
    "        for m in self.metrics: assert isinstance(m, Metric)\n",
    "        test_eq(self.recorder.smooth_loss.val, 0.)\n",
    "        #To test what the recorder logs, we use a custom logger function.\n",
    "        self.learn.logger = self.test_log\n",
    "        self.old_smooth,self.count = tensor(0.),0\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if self.training:\n",
    "            self.count += 1\n",
    "            test_eq(len(self.recorder.lrs), self.count)\n",
    "            test_eq(self.recorder.lrs[-1], self.opt.hypers[-1]['lr'])\n",
    "            test_eq(len(self.recorder.losses), self.count)\n",
    "            smooth = (1 - self.beta**(self.count-1)) * self.old_smooth * self.beta + self.loss * (1-self.beta)\n",
    "            smooth /= 1 - self.beta**self.count\n",
    "            test_close(self.recorder.losses[-1], smooth, eps=1e-4)\n",
    "            test_close(self.smooth_loss, smooth, eps=1e-4)\n",
    "            self.old_smooth = self.smooth_loss\n",
    "        self.bs += find_bs(self.yb)\n",
    "        test_eq(self.recorder.loss.count, self.bs)\n",
    "        if self.train_metrics or not self.training: \n",
    "            for m in self.metrics: test_eq(m.count, self.bs)\n",
    "        self.losses.append(self.loss.detach().cpu())\n",
    "    \n",
    "    def begin_epoch(self): \n",
    "        if self.add_time: self.start_epoch = time.time()\n",
    "        self.log = [self.epoch]\n",
    "    \n",
    "    def begin_train(self):\n",
    "        self.bs = 0\n",
    "        self.losses = []\n",
    "        for m in self.recorder._train_mets: test_eq(m.count, self.bs)\n",
    "            \n",
    "    def after_train(self):\n",
    "        res = tensor(self.losses).mean()\n",
    "        self.log += [res, res] if self.train_metrics else [res]\n",
    "        test_eq(self.log, self.recorder.log)\n",
    "        self.losses = []\n",
    "    \n",
    "    def begin_validate(self):\n",
    "        self.bs = 0\n",
    "        self.losses = []\n",
    "        for m in [self.recorder.loss] + self.metrics: test_eq(m.count, self.bs)\n",
    "    \n",
    "    def test_log(self, log):\n",
    "        res = tensor(self.losses).mean()\n",
    "        self.log += [res, res]\n",
    "        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n",
    "        test_eq(log, self.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "learn = synth_learner(n_train=5, metrics = tst_metric, cb_funcs = TestRecorderCallback)\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, ['epoch', 'train_loss', 'valid_loss', 'tst_metric', 'time'])\n",
    "\n",
    "learn = synth_learner(n_train=5, metrics = tst_metric, cb_funcs = TestRecorderCallback)\n",
    "learn.recorder.train_metrics=True\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, \n",
    "        ['epoch', 'train_loss', 'train_tst_metric', 'valid_loss', 'valid_tst_metric', 'time'])\n",
    "\n",
    "learn = synth_learner(n_train=5, metrics = tst_metric, cb_funcs = TestRecorderCallback)\n",
    "learn.recorder.add_time=False\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, ['epoch', 'train_loss', 'valid_loss', 'tst_metric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#5) [0,15.124231338500977,15.770105361938477,15.770104885101318,00:00]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#Test numpy metric\n",
    "def tst_metric_np(out, targ): return F.mse_loss(out, targ).numpy()\n",
    "learn = synth_learner(n_train=5, metrics=tst_metric_np)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Recorder.begin_fit\" class=\"doc_header\"><code>Recorder.begin_fit</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Recorder.begin_fit</code>()\n",
       "\n",
       "Prepare state for training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Recorder.begin_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Recorder.begin_epoch\" class=\"doc_header\"><code>Recorder.begin_epoch</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L30\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Recorder.begin_epoch</code>()\n",
       "\n",
       "Set timer if `self.add_time=True`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Recorder.begin_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Recorder.begin_validate\" class=\"doc_header\"><code>Recorder.begin_validate</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L37\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Recorder.begin_validate</code>()\n",
       "\n",
       "Reset loss and metrics state"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Recorder.begin_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Recorder.after_batch\" class=\"doc_header\"><code>Recorder.after_batch</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Recorder.after_batch</code>()\n",
       "\n",
       "Update all metrics and records lr and smooth loss in training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Recorder.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Recorder.after_epoch\" class=\"doc_header\"><code>Recorder.after_epoch</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Recorder.after_epoch</code>()\n",
       "\n",
       "Store and log the loss/metric values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Recorder.after_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Recorder.plot_loss\" class=\"doc_header\"><code>Recorder.plot_loss</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L59\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Recorder.plot_loss</code>(**`skip_start`**=*`5`*)\n",
       "\n",
       "Plot the losses from `skip_start` and onward"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Recorder.plot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "learn.recorder.plot_loss(skip_start=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.no_logging\" class=\"doc_header\"><code>Learner.no_logging</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L149\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.no_logging</code>()\n",
       "\n",
       "Context manager to temporarily remove `logger`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.no_logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = synth_learner(n_train=5, metrics=tst_metric)\n",
    "with learn.no_logging():\n",
    "    test_stdout(lambda: learn.fit(1), '')\n",
    "test_eq(learn.logger, print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.validate\" class=\"doc_header\"><code>Learner.validate</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L110\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.validate</code>(**`ds_idx`**=*`1`*, **`dl`**=*`None`*, **`cbs`**=*`None`*)\n",
       "\n",
       "Validate on `dl` with potential new `cbs`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test result\n",
    "learn = synth_learner(n_train=5, metrics=tst_metric)\n",
    "res = learn.validate()\n",
    "test_eq(res[0], res[1])\n",
    "x,y = learn.dbunch.valid_ds.tensors\n",
    "test_close(res[0], F.mse_loss(learn.model(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test other dl\n",
    "res = learn.validate(dl=learn.dbunch.train_dl)\n",
    "test_eq(res[0], res[1])\n",
    "x,y = learn.dbunch.train_ds.tensors\n",
    "test_close(res[0], F.mse_loss(learn.model(x), y))\n",
    "\n",
    "#Test additional callback is executed.\n",
    "cycle = cycle_events[:2] + ['begin_validate'] + batchv_events * 2 + cycle_events[-3:]\n",
    "test_stdout(lambda: learn.validate(cbs=VerboseCallback()), '\\n'.join(cycle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.loss_not_reduced\" class=\"doc_header\"><code>Learner.loss_not_reduced</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L152\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.loss_not_reduced</code>()\n",
       "\n",
       "A context manager to evaluate `loss_func` with reduction set to none."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.loss_not_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(learn.loss_func.reduction, 'mean')\n",
    "with learn.loss_not_reduced():\n",
    "    test_eq(learn.loss_func.reduction, 'none')\n",
    "    x,y = learn.dbunch.one_batch()\n",
    "    p = learn.model(x)\n",
    "    losses = learn.loss_func(p, y)\n",
    "    test_eq(losses.shape, y.shape)\n",
    "    test_eq(losses, F.mse_loss(p,y, reduction='none'))\n",
    "test_eq(learn.loss_func.reduction, 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.get_preds\" class=\"doc_header\"><code>Learner.get_preds</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L119\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.get_preds</code>(**`ds_idx`**=*`1`*, **`dl`**=*`None`*, **`with_input`**=*`False`*, **`with_loss`**=*`False`*, **`decoded`**=*`False`*, **`act`**=*`None`*)\n",
       "\n",
       "Get the predictions and targets on the `ds_idx`-th dbunchset or `dl`, optionally `with_input` and `with_loss`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.get_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the `loss_func` attribute of `Learner`, an activation function will be picked automatically so that the predictions make sense. For instance if the loss is a case of cross-entropy, a softmax will be applied, or if the loss is binary cross entropy with logits, a sigmoid will be applied. If you want to make sure a certain activation function is applied, you can pass it with `act`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: If you want to use the option `with_loss=True` on a custom loss function, make sure you have implemented a `reduction` attribute that supports 'none' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test result\n",
    "learn = synth_learner(n_train=5, metrics=tst_metric)\n",
    "preds,targs = learn.get_preds()\n",
    "x,y = learn.dbunch.valid_ds.tensors\n",
    "test_eq(targs, y)\n",
    "test_close(preds, learn.model(x))\n",
    "\n",
    "preds,targs = learn.get_preds(act = torch.sigmoid)\n",
    "test_eq(targs, y)\n",
    "test_close(preds, torch.sigmoid(learn.model(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test get_preds work with ds not evenly dividble by bs\n",
    "learn = synth_learner(n_train=2.5, metrics=tst_metric)\n",
    "preds,targs = learn.get_preds(ds_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test other dataset\n",
    "x = torch.randn(16*5)\n",
    "y = 2*x + 3 + 0.1*torch.randn(16*5)\n",
    "dl = TfmdDL(TensorDataset(x, y), bs=16)\n",
    "preds,targs = learn.get_preds(dl=dl)\n",
    "test_eq(targs, y)\n",
    "test_close(preds, learn.model(x))\n",
    "\n",
    "#Test with loss\n",
    "preds,targs,losses = learn.get_preds(dl=dl, with_loss=True)\n",
    "test_eq(targs, y)\n",
    "test_close(preds, learn.model(x))\n",
    "test_close(losses, F.mse_loss(preds, targs, reduction='none'))\n",
    "\n",
    "#Test with inputs\n",
    "inps,preds,targs = learn.get_preds(dl=dl, with_input=True)\n",
    "test_eq(*inps,x)\n",
    "test_eq(targs, y)\n",
    "test_close(preds, learn.model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test with no target\n",
    "learn = synth_learner(n_train=5)\n",
    "x = torch.randn(16*5)\n",
    "dl = TfmdDL(TensorDataset(x), bs=16)\n",
    "preds,targs = learn.get_preds(dl=dl)\n",
    "assert targs is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test with targets that are tuples\n",
    "def _fake_loss(x,y,z,reduction=None): return F.mse_loss(x,y)\n",
    "\n",
    "learn = synth_learner(n_train=5)\n",
    "x = torch.randn(16*5)\n",
    "y = 2*x + 3 + 0.1*torch.randn(16*5)\n",
    "learn.dbunch.n_inp=1\n",
    "learn.loss_func = _fake_loss\n",
    "dl = TfmdDL(TensorDataset(x, y, y), bs=16)\n",
    "preds,targs = learn.get_preds(dl=dl)\n",
    "test_eq(targs, [y,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test with inputs that are tuples\n",
    "class _TupleModel(Module):\n",
    "    def __init__(self, model): self.model=model\n",
    "    def forward(self, x1, x2): return self.model(x1)\n",
    "\n",
    "learn = synth_learner(n_train=5)\n",
    "#learn.dbunch.n_inp=2\n",
    "x = torch.randn(16*5)\n",
    "y = 2*x + 3 + 0.1*torch.randn(16*5)\n",
    "learn.model = _TupleModel(learn.model)\n",
    "learn.dbunch = DataBunch(TfmdDL(TensorDataset(x, x, y), bs=16),TfmdDL(TensorDataset(x, x, y), bs=16))\n",
    "inps,preds,targs = learn.get_preds(ds_idx=0, with_input=True)\n",
    "test_eq(inps, [x,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test auto activation function is picked\n",
    "learn = synth_learner(n_train=5)\n",
    "learn.loss_func = BCEWithLogitsLossFlat()\n",
    "x = torch.randn(16*5)\n",
    "y = 2*x + 3 + 0.1*torch.randn(16*5)\n",
    "dl = TfmdDL(TensorDataset(x, y), bs=16)\n",
    "preds,targs = learn.get_preds(dl=dl)\n",
    "test_close(preds, torch.sigmoid(learn.model(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.predict\" class=\"doc_header\"><code>Learner.predict</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L134\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.predict</code>(**`item`**)\n",
       "\n",
       "Return the prediction on `item`, fully decoded, loss function decoded and probabilities"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a tuple of three elements with, in reverse order,\n",
    "- the prediction from the model, potentially passed through the activation of the loss function (if it has one)\n",
    "- the decoded prediction, using the poential `decodes` method from it\n",
    "- the fully decoded prediction, using the transforms used to buil the `DataSource`/`DataBunch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _FakeLossFunc(Module):\n",
    "    reduction = 'none'\n",
    "    def forward(self, x, y): return F.mse_loss(x,y)\n",
    "    def activation(self, x): return x+1\n",
    "    def decodes(self, x):    return 2*x\n",
    "\n",
    "class _Add1(Transform):\n",
    "    def encodes(self, x): return x+1\n",
    "    def decodes(self, x): return x-1\n",
    "    \n",
    "learn = synth_learner(n_train=5)\n",
    "dl = TfmdDL(DataSource(torch.arange(50), tfms = [L(), [_Add1()]]))\n",
    "learn.dbunch = DataBunch(dl, dl)\n",
    "learn.loss_func = _FakeLossFunc()\n",
    "\n",
    "inp = tensor([2.])\n",
    "out = learn.model(inp).detach()+1  #applying model + activation\n",
    "dec = 2*out                        #decodes from loss function\n",
    "full_dec = dec-1                   #decodes from _Add1\n",
    "test_eq(learn.predict(tensor([2.])), [full_dec, dec, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def freeze_to(self:Learner, n):\n",
    "    if self.opt is None: self.create_opt()\n",
    "    self.opt.freeze_to(n)\n",
    "\n",
    "@patch\n",
    "def freeze(self:Learner): self.freeze_to(-1)\n",
    "\n",
    "@patch\n",
    "def unfreeze(self:Learner): self.freeze_to(0)    \n",
    "\n",
    "add_docs(Learner, \n",
    "         freeze_to=\"Freeze parameter groups up to `n`\",\n",
    "         freeze=\"Freeze up to last parameter group\",\n",
    "         unfreeze=\"Unfreeze the entire model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#4) [0,4.9132561683654785,3.1693623065948486,00:00]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "class _TstModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n",
    "        self.tst = nn.Sequential(nn.Linear(4,5), nn.BatchNorm1d(3))\n",
    "        self.tst[0].bias.data,self.tst[1].bias.data = torch.randn(5),torch.randn(3) \n",
    "    def forward(self, x): return x * self.a + self.b\n",
    "    \n",
    "class _PutGrad(Callback):\n",
    "    def after_backward(self):\n",
    "        for p in self.learn.model.tst.parameters():\n",
    "            if p.requires_grad: p.grad = torch.ones_like(p.data)\n",
    "\n",
    "def _splitter(m): return [list(m.tst[0].parameters()), list(m.tst[1].parameters()), [m.a,m.b]]\n",
    "            \n",
    "learn = synth_learner(n_train=5, opt_func = partial(SGD), cb_funcs=_PutGrad, splitter=_splitter, lr=1e-2)\n",
    "learn.model = _TstModel()\n",
    "learn.freeze()\n",
    "init = [p.clone() for p in learn.model.tst.parameters()]\n",
    "learn.fit(1)\n",
    "end = list(learn.model.tst.parameters())\n",
    "#linear was not trained\n",
    "for i in [0,1]: assert torch.allclose(end[i],init[i])\n",
    "#bn was trained even frozen since `train_bn=True` by default\n",
    "for i in [2,3]: assert torch.allclose(end[i]-init[i], -0.05 * torch.ones_like(end[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#4) [0,14.487665176391602,6.741222381591797,00:00]\n",
      "(#4) [0,11.724506378173828,5.451231956481934,00:00]\n",
      "(#4) [0,9.472497940063477,4.408669948577881,00:00]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "learn = synth_learner(n_train=5, opt_func = partial(SGD), cb_funcs=_PutGrad, splitter=_splitter, train_bn=False, lr=1e-2)\n",
    "learn.model = _TstModel()\n",
    "learn.freeze()\n",
    "init = [p.clone() for p in learn.model.tst.parameters()]\n",
    "learn.fit(1)\n",
    "end = list(learn.model.tst.parameters())\n",
    "#linear and bn were not trained\n",
    "for i in range(4): assert torch.allclose(end[i],init[i])\n",
    "\n",
    "learn.freeze_to(-2)\n",
    "init = [p.clone() for p in learn.model.tst.parameters()]\n",
    "learn.fit(1)\n",
    "end = list(learn.model.tst.parameters())\n",
    "#linear was not trained\n",
    "for i in [0,1]: assert torch.allclose(end[i],init[i])\n",
    "#bn was trained \n",
    "for i in [2,3]: assert torch.allclose(end[i]-init[i], -0.05 * torch.ones_like(end[i]))\n",
    "    \n",
    "learn.unfreeze()\n",
    "init = [p.clone() for p in learn.model.tst.parameters()]\n",
    "learn.fit(1)\n",
    "end = list(learn.model.tst.parameters())\n",
    "#linear and bn were trained\n",
    "for i in range(4): assert torch.allclose(end[i]-init[i], -0.05 * torch.ones_like(end[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
